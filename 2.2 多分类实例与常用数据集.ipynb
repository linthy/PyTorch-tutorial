{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自己动手编写一个3层全连接分类网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入了 CIFAR10 10分类数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.CIFAR10(r'C:\\Users\\Lin_hz\\data',train= True,transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.DataLoader(dataset,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,img_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(3*self.img_size*self.img_size,1024),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.Linear(1024,100),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.Linear(100,10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output = x.view(-1,3*self.img_size*self.img_size)\n",
    "        output = self.model(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    layer_name = m.__class__.__name__\n",
    "    if layer_name.find(\"Linear\") != -1:\n",
    "        nn.init.normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=1024, out_features=100, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "# 模型\n",
    "model = MLP(dataset[0][0].shape[1]).to(device)\n",
    "\n",
    "# 初始化\n",
    "model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9,weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,[10,],gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lin_hz\\Anaconda3\\envs\\env_new\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/12500 Loss: 5.5566911697387695]\n",
      "ETA 0:05:36.563482\n",
      "Train Epoch: 0 [2500/12500 Loss: 3.798368453979492]\n",
      "ETA 0:01:09.870055\n",
      "Train Epoch: 0 [5000/12500 Loss: 1.9861782789230347]\n",
      "ETA 0:00:52.466608\n",
      "Train Epoch: 0 [7500/12500 Loss: 1.9616122245788574]\n",
      "ETA 0:00:34.962119\n",
      "Train Epoch: 0 [10000/12500 Loss: 1.2703269720077515]\n",
      "ETA 0:00:17.460036\n",
      "Train Epoch: 1 [0/12500 Loss: 1.1804698705673218]\n",
      "ETA 0:01:52.509667\n",
      "Train Epoch: 1 [2500/12500 Loss: 2.259579658508301]\n",
      "ETA 0:01:09.299875\n",
      "Train Epoch: 1 [5000/12500 Loss: 2.3636789321899414]\n",
      "ETA 0:00:52.007491\n",
      "Train Epoch: 1 [7500/12500 Loss: 1.5589385032653809]\n",
      "ETA 0:00:34.742779\n",
      "Train Epoch: 1 [10000/12500 Loss: 2.263279914855957]\n",
      "ETA 0:00:17.376302\n",
      "Train Epoch: 2 [0/12500 Loss: 2.1833763122558594]\n",
      "ETA 0:01:52.172927\n",
      "Train Epoch: 2 [2500/12500 Loss: 2.215794801712036]\n",
      "ETA 0:01:09.363666\n",
      "Train Epoch: 2 [5000/12500 Loss: 1.770560622215271]\n",
      "ETA 0:00:52.152555\n",
      "Train Epoch: 2 [7500/12500 Loss: 1.5536837577819824]\n",
      "ETA 0:00:34.734139\n",
      "Train Epoch: 2 [10000/12500 Loss: 1.8661024570465088]\n",
      "ETA 0:00:17.371069\n",
      "Train Epoch: 3 [0/12500 Loss: 1.4649566411972046]\n",
      "ETA 0:01:39.728473\n",
      "Train Epoch: 3 [2500/12500 Loss: 2.320178270339966]\n",
      "ETA 0:01:10.679488\n",
      "Train Epoch: 3 [5000/12500 Loss: 1.8540589809417725]\n",
      "ETA 0:00:52.667008\n",
      "Train Epoch: 3 [7500/12500 Loss: 1.855692982673645]\n",
      "ETA 0:00:35.075777\n",
      "Train Epoch: 3 [10000/12500 Loss: 2.413097858428955]\n",
      "ETA 0:00:17.508383\n",
      "Train Epoch: 4 [0/12500 Loss: 1.6005146503448486]\n",
      "ETA 0:02:17.121436\n",
      "Train Epoch: 4 [2500/12500 Loss: 1.8626028299331665]\n",
      "ETA 0:01:09.475313\n",
      "Train Epoch: 4 [5000/12500 Loss: 1.9697684049606323]\n",
      "ETA 0:00:52.288645\n",
      "Train Epoch: 4 [7500/12500 Loss: 1.7267876863479614]\n",
      "ETA 0:00:34.912269\n",
      "Train Epoch: 4 [10000/12500 Loss: 2.9497411251068115]\n",
      "ETA 0:00:17.457793\n",
      "Train Epoch: 5 [0/12500 Loss: 2.4594883918762207]\n",
      "ETA 0:01:39.725493\n",
      "Train Epoch: 5 [2500/12500 Loss: 2.056136131286621]\n",
      "ETA 0:01:09.925897\n",
      "Train Epoch: 5 [5000/12500 Loss: 2.2896037101745605]\n",
      "ETA 0:00:52.533906\n",
      "Train Epoch: 5 [7500/12500 Loss: 1.701879620552063]\n",
      "ETA 0:00:34.892994\n",
      "Train Epoch: 5 [10000/12500 Loss: 2.043234348297119]\n",
      "ETA 0:00:17.415926\n",
      "Train Epoch: 6 [0/12500 Loss: 2.3561289310455322]\n",
      "ETA 0:01:52.187827\n",
      "Train Epoch: 6 [2500/12500 Loss: 1.8313754796981812]\n",
      "ETA 0:01:09.531135\n",
      "Train Epoch: 6 [5000/12500 Loss: 2.6758904457092285]\n",
      "ETA 0:00:52.252753\n",
      "Train Epoch: 6 [7500/12500 Loss: 2.963056802749634]\n",
      "ETA 0:00:34.824533\n",
      "Train Epoch: 6 [10000/12500 Loss: 1.7905446290969849]\n",
      "ETA 0:00:17.411191\n",
      "Train Epoch: 7 [0/12500 Loss: 2.0367789268493652]\n",
      "ETA 0:01:39.725493\n",
      "Train Epoch: 7 [2500/12500 Loss: 3.0371968746185303]\n",
      "ETA 0:01:10.089365\n",
      "Train Epoch: 7 [5000/12500 Loss: 2.168017864227295]\n",
      "ETA 0:00:52.536899\n",
      "Train Epoch: 7 [7500/12500 Loss: 1.207641363143921]\n",
      "ETA 0:00:34.968765\n",
      "Train Epoch: 7 [10000/12500 Loss: 1.34164559841156]\n",
      "ETA 0:00:17.480976\n",
      "Train Epoch: 8 [0/12500 Loss: 2.792419910430908]\n",
      "ETA 0:01:52.193787\n",
      "Train Epoch: 8 [2500/12500 Loss: 1.7052273750305176]\n",
      "ETA 0:01:09.941828\n",
      "Train Epoch: 8 [5000/12500 Loss: 2.1813571453094482]\n",
      "ETA 0:00:52.272194\n",
      "Train Epoch: 8 [7500/12500 Loss: 1.7986764907836914]\n",
      "ETA 0:00:34.900305\n",
      "Train Epoch: 8 [10000/12500 Loss: 1.785003662109375]\n",
      "ETA 0:00:17.465519\n",
      "Train Epoch: 9 [0/12500 Loss: 2.611654758453369]\n",
      "ETA 0:01:52.354707\n",
      "Train Epoch: 9 [2500/12500 Loss: 1.6236478090286255]\n",
      "ETA 0:01:10.113292\n",
      "Train Epoch: 9 [5000/12500 Loss: 1.074487566947937]\n",
      "ETA 0:00:52.364917\n",
      "Train Epoch: 9 [7500/12500 Loss: 2.2386717796325684]\n",
      "ETA 0:00:34.898312\n",
      "Train Epoch: 9 [10000/12500 Loss: 1.6732310056686401]\n",
      "ETA 0:00:17.412687\n",
      "Train Epoch: 10 [0/12500 Loss: 2.2214818000793457]\n",
      "ETA 0:01:52.062668\n",
      "Train Epoch: 10 [2500/12500 Loss: 2.1601972579956055]\n",
      "ETA 0:01:09.778310\n",
      "Train Epoch: 10 [5000/12500 Loss: 1.7005038261413574]\n",
      "ETA 0:00:52.465099\n",
      "Train Epoch: 10 [7500/12500 Loss: 1.8750081062316895]\n",
      "ETA 0:00:34.669659\n",
      "Train Epoch: 10 [10000/12500 Loss: 1.3863747119903564]\n",
      "ETA 0:00:17.332689\n",
      "Train Epoch: 11 [0/12500 Loss: 2.1915411949157715]\n",
      "ETA 0:01:52.196767\n",
      "Train Epoch: 11 [2500/12500 Loss: 2.325212240219116]\n",
      "ETA 0:01:07.836522\n",
      "Train Epoch: 11 [5000/12500 Loss: 1.1821844577789307]\n",
      "ETA 0:00:50.989057\n",
      "Train Epoch: 11 [7500/12500 Loss: 1.6472036838531494]\n",
      "ETA 0:00:34.005000\n",
      "Train Epoch: 11 [10000/12500 Loss: 2.8693580627441406]\n",
      "ETA 0:00:16.973581\n",
      "Train Epoch: 12 [0/12500 Loss: 1.3344001770019531]\n",
      "ETA 0:01:39.707613\n",
      "Train Epoch: 12 [2500/12500 Loss: 1.9525084495544434]\n",
      "ETA 0:01:07.768734\n",
      "Train Epoch: 12 [5000/12500 Loss: 0.8871908783912659]\n",
      "ETA 0:00:50.974101\n",
      "Train Epoch: 12 [7500/12500 Loss: 2.4890875816345215]\n",
      "ETA 0:00:34.008323\n",
      "Train Epoch: 12 [10000/12500 Loss: 3.103527784347534]\n",
      "ETA 0:00:17.018937\n",
      "Train Epoch: 13 [0/12500 Loss: 2.6235194206237793]\n",
      "ETA 0:01:52.193787\n",
      "Train Epoch: 13 [2500/12500 Loss: 1.4174799919128418]\n",
      "ETA 0:01:07.625191\n",
      "Train Epoch: 13 [5000/12500 Loss: 1.5469980239868164]\n",
      "ETA 0:00:50.917273\n",
      "Train Epoch: 13 [7500/12500 Loss: 1.4624607563018799]\n",
      "ETA 0:00:34.003006\n",
      "Train Epoch: 13 [10000/12500 Loss: 1.0216089487075806]\n",
      "ETA 0:00:17.002489\n",
      "Train Epoch: 14 [0/12500 Loss: 2.221142292022705]\n",
      "ETA 0:01:52.208687\n",
      "Train Epoch: 14 [2500/12500 Loss: 2.220787525177002]\n",
      "ETA 0:01:08.402721\n",
      "Train Epoch: 14 [5000/12500 Loss: 1.578223705291748]\n",
      "ETA 0:00:51.294139\n",
      "Train Epoch: 14 [7500/12500 Loss: 2.821180582046509]\n",
      "ETA 0:00:34.194431\n",
      "Train Epoch: 14 [10000/12500 Loss: 2.3580822944641113]\n",
      "ETA 0:00:17.068530\n",
      "Train Epoch: 15 [0/12500 Loss: 1.6835414171218872]\n",
      "ETA 0:01:39.701653\n",
      "Train Epoch: 15 [2500/12500 Loss: 1.4579167366027832]\n",
      "ETA 0:01:07.788664\n",
      "Train Epoch: 15 [5000/12500 Loss: 1.832909107208252]\n",
      "ETA 0:00:50.974099\n",
      "Train Epoch: 15 [7500/12500 Loss: 1.8843090534210205]\n",
      "ETA 0:00:34.100047\n",
      "Train Epoch: 15 [10000/12500 Loss: 2.227475643157959]\n",
      "ETA 0:00:17.036382\n",
      "Train Epoch: 16 [0/12500 Loss: 1.3333916664123535]\n",
      "ETA 0:01:52.190807\n",
      "Train Epoch: 16 [2500/12500 Loss: 1.5892430543899536]\n",
      "ETA 0:01:08.031898\n",
      "Train Epoch: 16 [5000/12500 Loss: 1.733253002166748]\n",
      "ETA 0:00:51.359940\n",
      "Train Epoch: 16 [7500/12500 Loss: 2.0788729190826416]\n",
      "ETA 0:00:34.181136\n",
      "Train Epoch: 16 [10000/12500 Loss: 1.9121040105819702]\n",
      "ETA 0:00:17.054574\n",
      "Train Epoch: 17 [0/12500 Loss: 1.4027477502822876]\n",
      "ETA 0:01:52.187827\n",
      "Train Epoch: 17 [2500/12500 Loss: 1.9603191614151]\n",
      "ETA 0:01:08.023919\n",
      "Train Epoch: 17 [5000/12500 Loss: 2.3885843753814697]\n",
      "ETA 0:00:51.012984\n",
      "Train Epoch: 17 [7500/12500 Loss: 1.7019931077957153]\n",
      "ETA 0:00:34.078777\n",
      "Train Epoch: 17 [10000/12500 Loss: 1.3123111724853516]\n",
      "ETA 0:00:17.055073\n",
      "Train Epoch: 18 [0/12500 Loss: 1.7221348285675049]\n",
      "ETA 0:01:39.707613\n",
      "Train Epoch: 18 [2500/12500 Loss: 2.0773255825042725]\n",
      "ETA 0:01:08.007973\n",
      "Train Epoch: 18 [5000/12500 Loss: 1.5161495208740234]\n",
      "ETA 0:00:51.110192\n",
      "Train Epoch: 18 [7500/12500 Loss: 1.341641902923584]\n",
      "ETA 0:00:34.041556\n",
      "Train Epoch: 18 [10000/12500 Loss: 2.9319047927856445]\n",
      "ETA 0:00:17.020682\n",
      "Train Epoch: 19 [0/12500 Loss: 1.6852272748947144]\n",
      "ETA 0:02:04.963061\n",
      "Train Epoch: 19 [2500/12500 Loss: 1.531337022781372]\n",
      "ETA 0:01:08.318988\n",
      "Train Epoch: 19 [5000/12500 Loss: 1.6258354187011719]\n",
      "ETA 0:00:51.273201\n",
      "Train Epoch: 19 [7500/12500 Loss: 1.2856268882751465]\n",
      "ETA 0:00:34.133945\n",
      "Train Epoch: 19 [10000/12500 Loss: 1.6914172172546387]\n",
      "ETA 0:00:17.063047\n",
      "Train Epoch: 20 [0/12500 Loss: 2.419489860534668]\n",
      "ETA 0:01:39.713573\n",
      "Train Epoch: 20 [2500/12500 Loss: 2.910067319869995]\n",
      "ETA 0:01:08.398734\n",
      "Train Epoch: 20 [5000/12500 Loss: 1.5356158018112183]\n",
      "ETA 0:00:51.131127\n",
      "Train Epoch: 20 [7500/12500 Loss: 3.0134503841400146]\n",
      "ETA 0:00:34.222345\n",
      "Train Epoch: 20 [10000/12500 Loss: 1.9871677160263062]\n",
      "ETA 0:00:17.094198\n",
      "Train Epoch: 21 [0/12500 Loss: 2.403535842895508]\n",
      "ETA 0:01:39.722513\n",
      "Train Epoch: 21 [2500/12500 Loss: 1.5396184921264648]\n",
      "ETA 0:01:08.626011\n",
      "Train Epoch: 21 [5000/12500 Loss: 1.689923644065857]\n",
      "ETA 0:00:51.117708\n",
      "Train Epoch: 21 [7500/12500 Loss: 1.2881776094436646]\n",
      "ETA 0:00:34.049532\n",
      "Train Epoch: 21 [10000/12500 Loss: 1.2722411155700684]\n",
      "ETA 0:00:17.024669\n",
      "Train Epoch: 22 [0/12500 Loss: 1.1073071956634521]\n",
      "ETA 0:02:05.112061\n",
      "Train Epoch: 22 [2500/12500 Loss: 2.0575335025787354]\n",
      "ETA 0:01:08.099685\n",
      "Train Epoch: 22 [5000/12500 Loss: 1.959758996963501]\n",
      "ETA 0:00:51.090751\n",
      "Train Epoch: 22 [7500/12500 Loss: 1.1116622686386108]\n",
      "ETA 0:00:34.133281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [10000/12500 Loss: 2.305758476257324]\n",
      "ETA 0:00:17.067533\n",
      "Train Epoch: 23 [0/12500 Loss: 1.877901554107666]\n",
      "ETA 0:02:17.124416\n",
      "Train Epoch: 23 [2500/12500 Loss: 1.5707371234893799]\n",
      "ETA 0:01:08.538289\n",
      "Train Epoch: 23 [5000/12500 Loss: 1.5081915855407715]\n",
      "ETA 0:00:51.506500\n",
      "Train Epoch: 23 [7500/12500 Loss: 1.793548345565796]\n",
      "ETA 0:00:34.244945\n",
      "Train Epoch: 23 [10000/12500 Loss: 2.269786834716797]\n",
      "ETA 0:00:17.109400\n",
      "Train Epoch: 24 [0/12500 Loss: 1.9803205728530884]\n",
      "ETA 0:01:52.077568\n",
      "Train Epoch: 24 [2500/12500 Loss: 2.4282217025756836]\n",
      "ETA 0:01:07.968066\n",
      "Train Epoch: 24 [5000/12500 Loss: 2.2217674255371094]\n",
      "ETA 0:00:51.202897\n",
      "Train Epoch: 24 [7500/12500 Loss: 1.2475064992904663]\n",
      "ETA 0:00:34.127293\n",
      "Train Epoch: 24 [10000/12500 Loss: 1.9897000789642334]\n",
      "ETA 0:00:17.053077\n",
      "Train Epoch: 25 [0/12500 Loss: 2.5246143341064453]\n",
      "ETA 0:01:52.184847\n",
      "Train Epoch: 25 [2500/12500 Loss: 2.2933335304260254]\n",
      "ETA 0:01:07.848490\n",
      "Train Epoch: 25 [5000/12500 Loss: 2.156374931335449]\n",
      "ETA 0:00:51.214878\n",
      "Train Epoch: 25 [7500/12500 Loss: 2.0830249786376953]\n",
      "ETA 0:00:34.115998\n",
      "Train Epoch: 25 [10000/12500 Loss: 1.0393065214157104]\n",
      "ETA 0:00:17.067782\n",
      "Train Epoch: 26 [0/12500 Loss: 1.496124505996704]\n",
      "ETA 0:01:52.187827\n",
      "Train Epoch: 26 [2500/12500 Loss: 2.4239113330841064]\n",
      "ETA 0:01:08.354867\n",
      "Train Epoch: 26 [5000/12500 Loss: 1.729027271270752]\n",
      "ETA 0:00:51.322550\n",
      "Train Epoch: 26 [7500/12500 Loss: 1.872989296913147]\n",
      "ETA 0:00:34.187118\n",
      "Train Epoch: 26 [10000/12500 Loss: 2.0539181232452393]\n",
      "ETA 0:00:17.108403\n",
      "Train Epoch: 27 [0/12500 Loss: 1.5573623180389404]\n",
      "ETA 0:02:04.653141\n",
      "Train Epoch: 27 [2500/12500 Loss: 1.6213030815124512]\n",
      "ETA 0:01:07.976076\n",
      "Train Epoch: 27 [5000/12500 Loss: 1.5790166854858398]\n",
      "ETA 0:00:51.020460\n",
      "Train Epoch: 27 [7500/12500 Loss: 1.6483887434005737]\n",
      "ETA 0:00:34.013639\n",
      "Train Epoch: 27 [10000/12500 Loss: 1.6376471519470215]\n",
      "ETA 0:00:17.009218\n",
      "Train Epoch: 28 [0/12500 Loss: 1.6177661418914795]\n",
      "ETA 0:01:39.710593\n",
      "Train Epoch: 28 [2500/12500 Loss: 1.5421432256698608]\n",
      "ETA 0:01:09.774355\n",
      "Train Epoch: 28 [5000/12500 Loss: 2.3826088905334473]\n",
      "ETA 0:00:51.937201\n",
      "Train Epoch: 28 [7500/12500 Loss: 1.6827583312988281]\n",
      "ETA 0:00:34.478241\n",
      "Train Epoch: 28 [10000/12500 Loss: 1.703819751739502]\n",
      "ETA 0:00:17.210329\n",
      "Train Epoch: 29 [0/12500 Loss: 1.0588901042938232]\n",
      "ETA 0:01:39.662913\n",
      "Train Epoch: 29 [2500/12500 Loss: 1.7176389694213867]\n",
      "ETA 0:01:08.350865\n",
      "Train Epoch: 29 [5000/12500 Loss: 1.6555383205413818]\n",
      "ETA 0:00:51.312077\n",
      "Train Epoch: 29 [7500/12500 Loss: 2.009031057357788]\n",
      "ETA 0:00:34.290802\n",
      "Train Epoch: 29 [10000/12500 Loss: 1.2743761539459229]\n",
      "ETA 0:00:17.110146\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(30):\n",
    "    scheduler.step()\n",
    "    start_time = time.time()\n",
    "    for i,(img,label) in enumerate(train_data):\n",
    "        img = img.requires_grad_(True).to(device)\n",
    "        label = torch.tensor(label).to(device)\n",
    "        output = model(img)\n",
    "        loss = loss_func(output,label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        time_left = datetime.timedelta(seconds=(len(train_data)-(i+1)) * (time.time() - start_time) / (i + 1))\n",
    "        \n",
    "        if (i+1) % 2500 == 0 :\n",
    "            print(f'Train Epoch: {epoch_i} [{i}/{len(train_data)} Loss: {loss.data}]')\n",
    "            print(f'ETA {time_left}'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4690, -2.3412,  0.8872,  0.6097,  0.8977,  0.8558,  1.8368,  0.5649,\n",
       "         -1.5599, -1.3189],\n",
       "        [ 2.9101, -0.6749,  0.3035, -0.7280, -0.4876, -1.0610, -1.9720, -0.7218,\n",
       "          1.1142, -0.9825],\n",
       "        [ 0.2765,  0.8876, -0.5191,  0.0599, -0.6984, -0.4116, -2.3738,  0.2197,\n",
       "         -0.4269,  1.4753],\n",
       "        [-1.4414,  1.9443, -0.1751,  0.3604,  0.4662,  0.4390,  1.7379,  0.0072,\n",
       "          0.1391,  0.6935]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = F.softmax(output,dim=1).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(pre,label).sum().float().item() / 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = datasets.CIFAR10(r'C:\\Users\\Lin_hz\\data',train= False,transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]))\n",
    "test_data = torch.utils.data.DataLoader(dataset,batch_size=4,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lin_hz\\Anaconda3\\envs\\env_new\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test [2500/12500] Loss: 1.8524 Accuracy:  34.08%\n",
      "Test [5000/12500] Loss: 1.8613 Accuracy:  33.92%\n",
      "Test [7500/12500] Loss: 1.8588 Accuracy:  34.01%\n",
      "Test [10000/12500] Loss: 1.8602 Accuracy:  34.08%\n",
      "Test [12500/12500] Loss: 1.8640 Accuracy:  33.95%\n"
     ]
    }
   ],
   "source": [
    "losses = 0\n",
    "accuracys = 0\n",
    "for i,(img,label) in enumerate(test_data):\n",
    "        img = img.requires_grad_(True).to(device)\n",
    "        label = torch.tensor(label).to(device)\n",
    "        output = model(img)\n",
    "        loss = loss_func(output,label)\n",
    "        losses += loss\n",
    "        \n",
    "        pred = F.softmax(output,dim=1).argmax(dim=1)\n",
    "        accu = torch.eq(pred,label).sum().float().item() / len(label)\n",
    "        accuracys += accu\n",
    "        if (i+1) % 2500 == 0:\n",
    "            print(f'Test [{i+1}/{len(test_data)}] Loss: {losses.item()/ (i+1):2.4f}\\\n",
    " Accuracy: {accuracys/(i+1)*100 : 2.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
